ä»é›¶å¼€å§‹æ„å»ºé«˜è´¨é‡æ¨¡å‹

---

ğŸ“Š æ€»ä½“æµç¨‹æ¦‚è§ˆ

é˜¶æ®µ1: é—®é¢˜å®šä¹‰ â†’ é˜¶æ®µ2: æ•°æ®æ”¶é›† â†’ é˜¶æ®µ3: æ•°æ®é¢„å¤„ç†
â†“              â†“                  â†“
é˜¶æ®µ4: æ¢ç´¢æ€§åˆ†æ â†’ é˜¶æ®µ5: ç‰¹å¾å·¥ç¨‹ â†’ é˜¶æ®µ6: æ¨¡å‹é€‰æ‹©
â†“              â†“                  â†“
é˜¶æ®µ7: æ¨¡å‹è®­ç»ƒ â†’ é˜¶æ®µ8: è¯„ä¼°ä¼˜åŒ– â†’ é˜¶æ®µ9: éƒ¨ç½²ç›‘æ§

---

ğŸ” é˜¶æ®µ1: é—®é¢˜å®šä¹‰ï¼ˆæœ€é‡è¦å´è¢«å¿½è§†çš„ç¯èŠ‚ï¼‰

æ ¸å¿ƒä»»åŠ¡

- æ˜ç¡®ä¸šåŠ¡ç›®æ ‡ï¼ˆé¢„æµ‹ï¼Ÿåˆ†ç±»ï¼Ÿèšç±»ï¼Ÿï¼‰
- ç¡®å®šé—®é¢˜ç±»å‹ï¼ˆç›‘ç£/æ— ç›‘ç£/å¼ºåŒ–å­¦ä¹ ï¼‰
- å®šä¹‰è¯„ä¼°æ ‡å‡†ï¼ˆå‡†ç¡®ç‡ï¼Ÿåˆ©æ¶¦ï¼Ÿç”¨æˆ·æ»¡æ„åº¦ï¼Ÿï¼‰

æ¶‰åŠç®—æ³•/æŠ€æœ¯


| é—®é¢˜ç±»å‹   | é€‚ç”¨åœºæ™¯            | ç¤ºä¾‹                             |
| ---------- | ------------------- | -------------------------------- |
| ç›‘ç£å­¦ä¹    | æœ‰æ ‡ç­¾æ•°æ®          | æˆ¿ä»·é¢„æµ‹ã€ç–¾ç—…è¯Šæ–­ã€åƒåœ¾é‚®ä»¶è¯†åˆ« |
| æ— ç›‘ç£å­¦ä¹  | æ— æ ‡ç­¾æ•°æ®æ¢ç´¢      | å®¢æˆ·åˆ†ç¾¤ã€å¼‚å¸¸æ£€æµ‹ã€é™ç»´å¯è§†åŒ–   |
| åŠç›‘ç£å­¦ä¹  | å°‘é‡æ ‡ç­¾+å¤§é‡æ— æ ‡ç­¾ | åŒ»å­¦å½±åƒæ ‡æ³¨                     |
| å¼ºåŒ–å­¦ä¹    | å†³ç­–ä¼˜åŒ–            | æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ã€æ¨èç³»ç»Ÿ       |

å®æˆ˜ç¤ºä¾‹ï¼šè´«è¡€è¯Šæ–­é¡¹ç›®

# âŒ æ¨¡ç³Šçš„é—®é¢˜å®šä¹‰

"ç”¨æœºå™¨å­¦ä¹ è§£å†³è´«è¡€é—®é¢˜"

# âœ… æ¸…æ™°çš„é—®é¢˜å®šä¹‰

"æ„å»ºå¤šåˆ†ç±»æ¨¡å‹ï¼Œæ ¹æ®24é¡¹è¡€æ¶²æŒ‡æ ‡é¢„æµ‹5ç§è´«è¡€ç±»å‹
è¯„ä¼°æŒ‡æ ‡ï¼šMacro F1 > 95%ï¼ˆå¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼‰
ä¸šåŠ¡ç›®æ ‡ï¼šè¾…åŠ©åŒ»ç”Ÿè¯Šæ–­ï¼Œå‡å°‘è¯¯è¯Šç‡"

---

ğŸ“¥ é˜¶æ®µ2: æ•°æ®æ”¶é›†

æ ¸å¿ƒä»»åŠ¡

- ç¡®å®šæ•°æ®æºï¼ˆæ•°æ®åº“ã€APIã€æ–‡ä»¶ã€çˆ¬è™«ï¼‰
- æ”¶é›†åŸå§‹æ•°æ®
- ç¡®ä¿æ•°æ®è´¨é‡å’Œåˆæ³•æ€§

æ¶‰åŠç®—æ³•/æŠ€æœ¯

2.1 æ•°æ®æºè·å–

# 1. æ–‡ä»¶è¯»å–

import pandas as pd
df = pd.read_csv("data.csv")
df = pd.read_excel("data.xlsx")

# 2. æ•°æ®åº“æŸ¥è¯¢

import sqlite3
conn = sqlite3.connect("database.db")
df = pd.read_sql("SELECT * FROM patients", conn)

# 3. APIè·å–

import requests
response = requests.get("https://api.example.com/data")
data = response.json()
df = pd.DataFrame(data)

# 4. ç½‘ç»œçˆ¬è™«ï¼ˆéœ€è°¨æ…ï¼‰

from bs4 import BeautifulSoup
import requests

2.2 æ•°æ®åˆæ³•æ€§

- GDPR/CCPA ç”¨æˆ·éšç§ä¿æŠ¤æ³•è§„
- æ•°æ®æˆæƒåè®®ï¼ˆKaggleæ•°æ®é›†é€šå¸¸å·²æˆæƒï¼‰
- ä¼¦ç†å®¡æŸ¥ï¼šåŒ»å­¦æ•°æ®éœ€è„±æ•å¤„ç†

---

ğŸ§¹ é˜¶æ®µ3: æ•°æ®é¢„å¤„ç†ï¼ˆå é¡¹ç›®70%æ—¶é—´ï¼‰

æ ¸å¿ƒä»»åŠ¡

- å¤„ç†ç¼ºå¤±å€¼
- å¤„ç†å¼‚å¸¸å€¼
- æ•°æ®ç±»å‹è½¬æ¢
- æ•°æ®æ¸…æ´—

æ¶‰åŠç®—æ³•/æŠ€æœ¯

3.1 ç¼ºå¤±å€¼å¤„ç†

import pandas as pd
import numpy as np

# 1. æ£€æµ‹ç¼ºå¤±å€¼

df.isnull().sum()

# 2. åˆ é™¤æ³•ï¼ˆé€‚ç”¨äºç¼ºå¤±<5%ï¼‰

df.dropna(subset=['important_column'], inplace=True)

# 3. å¡«å……æ³•

# 3.1 å‡å€¼/ä¸­ä½æ•°/ä¼—æ•°å¡«å……ï¼ˆæ•°å€¼å‹ï¼‰

df['age'].fillna(df['age'].median(), inplace=True)

# 3.2 å‰å‘/åå‘å¡«å……ï¼ˆæ—¶é—´åºåˆ—ï¼‰

df['value'].fillna(method='ffill', inplace=True)

# 3.3 Kè¿‘é‚»å¡«å……ï¼ˆä¿æŒåˆ†å¸ƒï¼‰

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# 3.4 æ¨¡å‹é¢„æµ‹å¡«å……ï¼ˆæœ€ç²¾ç¡®ä½†å¤æ‚ï¼‰

from sklearn.ensemble import RandomForestRegressor

# ç”¨å…¶ä»–ç‰¹å¾é¢„æµ‹ç¼ºå¤±å€¼

3.2 å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†

ç»Ÿè®¡æ–¹æ³•ï¼š

# 1. Z-scoreï¼ˆæ ‡å‡†å·®æ³•ï¼‰

from scipy import stats
z_scores = np.abs(stats.zscore(df['age']))
df = df[z_scores < 3]  # ä¿ç•™3ä¸ªæ ‡å‡†å·®å†…çš„æ•°æ®

# 2. IQRï¼ˆå››åˆ†ä½æ•°æ³•ï¼‰

Q1 = df['age'].quantile(0.25)
Q3 = df['age'].quantile(0.75)
IQR = Q3 - Q1
df = df[~((df['age'] < (Q1 - 1.5 * IQR)) | (df['age'] > (Q3 + 1.5 * IQR)))]

# 3. Isolation Forestï¼ˆæœºå™¨å­¦ä¹ æ–¹æ³•ï¼‰

from sklearn.ensemble import IsolationForest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
outliers = iso_forest.fit_predict(df)
df = df[outliers == 1]  # åªä¿ç•™æ­£å¸¸ç‚¹

å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥ï¼š


| ç­–ç•¥         | é€‚ç”¨åœºæ™¯                     | ä¼˜ç¼ºç‚¹             |
| ------------ | ---------------------------- | ------------------ |
| åˆ é™¤         | æ˜æ˜¾é”™è¯¯æ•°æ®ï¼ˆå¦‚å¹´é¾„=250ï¼‰   | ç®€å•ä½†å¯èƒ½ä¸¢å¤±ä¿¡æ¯ |
| æˆªæ–­ï¼ˆClipï¼‰ | åˆç†èŒƒå›´å¤–çš„æç«¯å€¼           | ä¿ç•™æ ·æœ¬ä½†æ”¹å˜åˆ†å¸ƒ |
| ä¿ç•™         | çœŸå®ç½•è§æƒ…å†µï¼ˆå¦‚åŒ»å­¦å¼‚å¸¸å€¼ï¼‰ | ä¿ç•™åŸå§‹ä¿¡æ¯       |
| æ ‡è®°         | ä¸ç¡®å®šæ€§é«˜æ—¶                 | ä¸åˆ é™¤ä½†å•ç‹¬æ ‡æ³¨   |

---

ğŸ“ˆ é˜¶æ®µ4: æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰

æ ¸å¿ƒä»»åŠ¡

- ç†è§£æ•°æ®åˆ†å¸ƒ
- å‘ç°æ•°æ®æ¨¡å¼
- è¯†åˆ«é—®é¢˜
- éªŒè¯å‡è®¾

æ¶‰åŠç®—æ³•/æŠ€æœ¯

4.1 æè¿°æ€§ç»Ÿè®¡

# åŸºæœ¬ç»Ÿè®¡

df.describe()  # å‡å€¼ã€æ ‡å‡†å·®ã€å››åˆ†ä½æ•°

# ç±»åˆ«åˆ†å¸ƒ

df['target'].value_counts()

# ç›¸å…³æ€§åˆ†æ

df.corr()  # çš®å°”é€Šç›¸å…³ç³»æ•°çŸ©é˜µ

4.2 å¯è§†åŒ–åˆ†æ

åˆ†å¸ƒå¯è§†åŒ–ï¼š
import matplotlib.pyplot as plt
import seaborn as sns

# 1. ç›´æ–¹å›¾ï¼ˆå•å˜é‡åˆ†å¸ƒï¼‰

plt.figure(figsize=(10, 6))
plt.hist(df['age'], bins=30, edgecolor='black')
plt.title('å¹´é¾„åˆ†å¸ƒç›´æ–¹å›¾')
plt.xlabel('å¹´é¾„')
plt.ylabel('é¢‘æ•°')
plt.show()

# 2. KDEå›¾ï¼ˆæ ¸å¯†åº¦ä¼°è®¡ï¼Œæ›´å¹³æ»‘ï¼‰

sns.kdeplot(df['age'], shade=True)
plt.title('å¹´é¾„æ ¸å¯†åº¦åˆ†å¸ƒ')
plt.show()

# 3. ç®±çº¿å›¾ï¼ˆè¯†åˆ«å¼‚å¸¸å€¼å’Œåˆ†å¸ƒï¼‰

plt.figure(figsize=(10, 6))
sns.boxplot(y='age', data=df)
plt.title('å¹´é¾„ç®±çº¿å›¾')
plt.show()

å…³ç³»å¯è§†åŒ–ï¼š

# 4. æ•£ç‚¹å›¾ï¼ˆåŒå˜é‡å…³ç³»ï¼‰

plt.scatter(df['age'], df['blood_pressure'])
plt.xlabel('å¹´é¾„')
plt.ylabel('è¡€å‹')
plt.title('å¹´é¾„ vs è¡€å‹æ•£ç‚¹å›¾')
plt.show()

# 5. çƒ­åŠ›å›¾ï¼ˆå¤šå˜é‡ç›¸å…³æ€§ï¼‰

plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')
plt.show()

# 6. é…å¯¹å›¾ï¼ˆå¤šå˜é‡å…³ç³»çŸ©é˜µï¼‰

sns.pairplot(df, hue='target')  # ä¸åŒç±»åˆ«ç”¨ä¸åŒé¢œè‰²
plt.show()

é«˜çº§EDAï¼š

# 7. Q-Qå›¾ï¼ˆæ£€éªŒæ­£æ€åˆ†å¸ƒï¼‰

from scipy import stats
stats.probplot(df['age'], dist="norm", plot=plt)
plt.title('Q-Qå›¾ï¼ˆæ£€éªŒæ­£æ€åˆ†å¸ƒï¼‰')
plt.show()

# 8. å°æç´å›¾ï¼ˆåˆ†å¸ƒ+ç®±çº¿å›¾ç»“åˆï¼‰

sns.violinplot(x='gender', y='age', data=df, inner='quart')
plt.title('æŒ‰æ€§åˆ«åˆ†ç»„çš„å¹´é¾„åˆ†å¸ƒå°æç´å›¾')
plt.show()

# 9. æ¡å½¢å›¾ï¼ˆç±»åˆ«å¯¹æ¯”ï¼‰

category_means = df.groupby('category')['age'].mean()
category_means.plot(kind='bar')
plt.title('å„ç±»åˆ«å¹³å‡å¹´é¾„å¯¹æ¯”')
plt.ylabel('å¹³å‡å¹´é¾„')
plt.show()

---

ğŸ”§ é˜¶æ®µ5: ç‰¹å¾å·¥ç¨‹ï¼ˆå†³å®šæ¨¡å‹æ€§èƒ½çš„å…³é”®ï¼‰

æ ¸å¿ƒä»»åŠ¡

- ç‰¹å¾åˆ›é€ ï¼ˆç”Ÿæˆæ–°ç‰¹å¾ï¼‰
- ç‰¹å¾é€‰æ‹©ï¼ˆå»é™¤å†—ä½™ï¼‰
- ç‰¹å¾è½¬æ¢ï¼ˆè§„èŒƒåŒ–ï¼‰
- é™ç»´ï¼ˆå‡å°‘ç»´åº¦ï¼‰

æ¶‰åŠç®—æ³•/æŠ€æœ¯

5.1 ç‰¹å¾åˆ›é€ 

# 1. æ•°å­¦å˜æ¢

df['age_squared'] = df['age'] ** 2
df['log_income'] = np.log1p(df['income'])  # å¤„ç†å³ååˆ†å¸ƒ
df['bmi'] = df['weight'] / (df['height'] / 100) ** 2

# 2. äº¤äº’ç‰¹å¾

df['age_x_income'] = df['age'] * df['income']
df['blood_pressure_diff'] = df['systolic'] - df['diastolic']

# 3. åˆ†ç®±ï¼ˆBinningï¼Œå°†è¿ç»­å˜ç¦»æ•£ï¼‰

# ç­‰å®½åˆ†ç®±

df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 60, 100],
labels=['child', 'young', 'adult', 'senior'])

# ç­‰é¢‘åˆ†ç®±

df['income_quantile'] = pd.qcut(df['income'], q=5,
labels=['very_low', 'low', 'medium', 'high', 'very_high'])

# 4. ç¼–ç åˆ†ç±»å˜é‡

# ç‹¬çƒ­ç¼–ç ï¼ˆOne-Hotï¼‰

df = pd.get_dummies(df, columns=['gender', 'occupation'])

# æ ‡ç­¾ç¼–ç ï¼ˆLabel Encodingï¼‰

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['education_encoded'] = le.fit_transform(df['education'])

5.2 ç‰¹å¾é€‰æ‹©

è¿‡æ»¤æ³•ï¼ˆFilterï¼‰ï¼š
from sklearn.feature_selection import SelectKBest, f_classif

# 1. æ–¹å·®åˆ†æï¼ˆANOVA F-testï¼‰

selector = SelectKBest(score_func=f_classif, k=10)  # é€‰æ‹©æœ€å¥½çš„10ä¸ªç‰¹å¾
X_new = selector.fit_transform(X, y)

# æŸ¥çœ‹åˆ†æ•°

feature_scores = pd.DataFrame({
'feature': X.columns,
'score': selector.scores_,
'p_value': selector.pvalues_
}).sort_values('score', ascending=False)

# 2. ç›¸å…³ç³»æ•°æ³•ï¼ˆçš®å°”é€Šç›¸å…³ï¼‰

correlations = X.corrwith(y).abs().sort_values(ascending=False)
top_features = correlations.head(10).index

# 3. æ–¹å·®é˜ˆå€¼ï¼ˆå»é™¤ä½æ–¹å·®ç‰¹å¾ï¼‰

from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)  # æ–¹å·®<0.01çš„åˆ é™¤
X_new = selector.fit_transform(X)

åŒ…è£…æ³•ï¼ˆWrapperï¼‰ï¼š
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰

estimator = RandomForestClassifier(n_estimators=50)
selector = RFE(estimator, n_features_to_select=10, step=1)
selector = selector.fit(X, y)

# æŸ¥çœ‹è¢«é€‰ä¸­çš„ç‰¹å¾

selected_features = X.columns[selector.support_]

åµŒå…¥æ³•ï¼ˆEmbeddedï¼‰ï¼š
from sklearn.linear_model import LassoCV

# L1æ­£åˆ™åŒ–è‡ªåŠ¨ç‰¹å¾é€‰æ‹©

lasso = LassoCV(cv=5)
lasso.fit(X, y)

# ç³»æ•°ä¸º0çš„ç‰¹å¾è¢«è‡ªåŠ¨å‰”é™¤

selected_features = X.columns[lasso.coef_ != 0]

# éšæœºæ£®æ—å†…ç½®ç‰¹å¾é‡è¦æ€§

rf = RandomForestClassifier()
rf.fit(X, y)
importances = rf.feature_importances_

5.3 ç‰¹å¾è½¬æ¢

æ ‡å‡†åŒ–ï¼ˆStandardizationï¼‰ï¼š
from sklearn.preprocessing import StandardScaler

# Z-scoreæ ‡å‡†åŒ– (å‡å€¼=0, æ ‡å‡†å·®=1)

# é€‚ç”¨äºè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆSVMã€é€»è¾‘å›å½’ã€ç¥ç»ç½‘ç»œï¼‰

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# å…¬å¼: X_std = (X - X.mean()) / X.std()

å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰ï¼š
from sklearn.preprocessing import MinMaxScaler

# Min-Maxå½’ä¸€åŒ– (èŒƒå›´=[0,1])

# é€‚ç”¨äºç¥ç»ç½‘ç»œã€KNNç­‰è·ç¦»-basedç®—æ³•

scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

# å…¬å¼: X_scaled = (X - X.min()) / (X.max() - X.min())

é²æ£’æ ‡å‡†åŒ–ï¼ˆRobust Scalerï¼‰ï¼š
from sklearn.preprocessing import RobustScaler

# ä½¿ç”¨ä¸­ä½æ•°å’Œå››åˆ†ä½æ•°ï¼ˆå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰

# é€‚ç”¨äºæœ‰å¼‚å¸¸å€¼çš„æ•°æ®

scaler = RobustScaler()
X_robust = scaler.fit_transform(X)

# å…¬å¼: X_robust = (X - median) / IQR

5.4 é™ç»´æŠ€æœ¯

PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼š
from sklearn.decomposition import PCA

# 1. PCAé™ç»´

pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
X_pca = pca.fit_transform(X)

# æŸ¥çœ‹å„æˆåˆ†è§£é‡Šçš„æ–¹å·®

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
plt.ylabel('ç´¯è®¡è§£é‡Šæ–¹å·®')
plt.show()

# 2. æŸ¥çœ‹æˆåˆ†ç³»æ•°

pca_components = pd.DataFrame(
pca.components_,
columns=X.columns,
index=[f'PC{i+1}' for i in range(len(pca.components_))]
)

t-SNEï¼ˆéçº¿æ€§é™ç»´ï¼Œå¯è§†åŒ–ï¼‰ï¼š
from sklearn.manifold import TSNE

# t-SNEæ“…é•¿å¯è§†åŒ–é«˜ç»´æ•°æ®

# æ³¨æ„ï¼šä»…ç”¨äºå¯è§†åŒ–ï¼Œä¸é€‚åˆä½œä¸ºæ¨¡å‹è¾“å…¥

tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
plt.title('t-SNEé™ç»´å¯è§†åŒ–')
plt.show()

---

ğŸ›ï¸ é˜¶æ®µ6: æ¨¡å‹é€‰æ‹©

æ ¸å¿ƒä»»åŠ¡

- æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©åŸºçº¿æ¨¡å‹
- è®­ç»ƒå¤šä¸ªå€™é€‰æ¨¡å‹
- å¿«é€ŸåŸå‹éªŒè¯

æ¶‰åŠç®—æ³•/æŠ€æœ¯

6.1 ç›‘ç£å­¦ä¹ ç®—æ³•å¯¹æ¯”è¡¨


| ç®—æ³•       | é€‚ç”¨é—®é¢˜         | ä¼˜ç‚¹           | ç¼ºç‚¹       | å¤æ‚åº¦   |
| ---------- | ---------------- | -------------- | ---------- | -------- |
| é€»è¾‘å›å½’   | äºŒåˆ†ç±»ã€çº¿æ€§å¯åˆ† | å¿«é€Ÿã€å¯è§£é‡Š   | åªèƒ½çº¿æ€§   | â­       |
| å†³ç­–æ ‘     | åˆ†ç±»/å›å½’        | éçº¿æ€§ã€å¯è§†åŒ– | æ˜“è¿‡æ‹Ÿåˆ   | â­â­     |
| éšæœºæ£®æ—   | åˆ†ç±»/å›å½’        | å‡†ç¡®ã€æŠ—è¿‡æ‹Ÿåˆ | ä¸å¯è§£é‡Š   | â­â­â­   |
| æ¢¯åº¦æå‡   | åˆ†ç±»/å›å½’        | æœ€å‡†ç¡®         | è®­ç»ƒæ…¢     | â­â­â­â­ |
| SVM        | äºŒåˆ†ç±»ã€å¤æ‚è¾¹ç•Œ | æ³›åŒ–å¥½         | å†…å­˜æ¶ˆè€—å¤§ | â­â­â­   |
| KNN        | åˆ†ç±»/å›å½’        | ç®€å•           | é¢„æµ‹æ…¢     | â­â­     |
| æœ´ç´ è´å¶æ–¯ | æ–‡æœ¬åˆ†ç±»         | éå¸¸å¿«         | å‡è®¾ç‹¬ç«‹   | â­       |

å®æˆ˜ï¼šè´«è¡€è¯Šæ–­é¡¹ç›®æ¨¡å‹é€‰æ‹©

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# 6ç§å€™é€‰æ¨¡å‹

models = {
'Logistic Regression': LogisticRegression(
max_iter=1000, class_weight='balanced'
),
'Decision Tree': DecisionTreeClassifier(
class_weight='balanced'
),
'Random Forest': RandomForestClassifier(
n_estimators=100, class_weight='balanced'
),
'Gradient Boosting': GradientBoostingClassifier(),
'SVM': SVC(class_weight='balanced'),
'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)
}

# å¿«é€ŸåŸå‹è®­ç»ƒï¼ˆåªç”¨ä¸€ä¸ªç®€å•çš„è®­ç»ƒé›†ï¼‰

for name, model in models.items():
model.fit(X_train, y_train)
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)
print(f"{name}: Train={train_score:.4f}, Test={test_score:.4f}")

é¢„æœŸç»“æœåˆ†æï¼š

Logistic Regression: Train=0.85, Test=0.83
â†‘ çº¿æ€§è¾¹ç•Œï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ

Decision Tree: Train=1.00, Test=0.95
â†‘ å®Œç¾æ‹Ÿåˆè®­ç»ƒé›†ï¼Œè½»å¾®è¿‡æ‹Ÿåˆ

Random Forest: Train=0.98, Test=0.97
â†‘ è®­ç»ƒå¾—åˆ†é«˜ä½†ä¸è¿‡åˆ†ï¼Œç†æƒ³çŠ¶æ€ï¼

Gradient Boosting: Train=1.00, Test=0.99
â†‘ æœ€å‡†ç¡®ï¼Œä½†è®­ç»ƒæ—¶é—´æœ€é•¿

SVM: Train=0.95, Test=0.94
â†‘ è¡¨ç°ç¨³å®š

KNN: Train=0.99, Test=0.85
â†‘ ä¸¥é‡è¿‡æ‹Ÿåˆï¼å·®è·å¤ªå¤§

æ¨¡å‹é€‰æ‹©å†³ç­–ï¼š

- ğŸ† ä¸»æ¨¡å‹: Gradient Boostingï¼ˆæœ€ä¼˜æ€§èƒ½ï¼‰
- ğŸ¥ˆ å€™é€‰æ¨¡å‹: Random Forestï¼ˆé€Ÿåº¦å’Œå‡†ç¡®ç‡å¹³è¡¡ï¼‰
- ğŸ“Š åŸºå‡†: Logistic Regressionï¼ˆå¿«é€ŸåŸå‹ï¼‰

---

ğŸ‹ï¸ é˜¶æ®µ7: æ¨¡å‹è®­ç»ƒ

æ ¸å¿ƒä»»åŠ¡

- è®­ç»ƒæœ€ç»ˆæ¨¡å‹
- ä¼˜åŒ–è¶…å‚æ•°
- äº¤å‰éªŒè¯

æ¶‰åŠç®—æ³•/æŠ€æœ¯

7.1 äº¤å‰éªŒè¯ï¼ˆé‡è¦ï¼ï¼‰

KæŠ˜äº¤å‰éªŒè¯ï¼š
from sklearn.model_selection import KFold, StratifiedKFold

# é€šç”¨KæŠ˜ï¼ˆé€‚ç”¨äºå¹³è¡¡æ•°æ®ï¼‰

kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# åˆ†å±‚KæŠ˜ï¼ˆé€‚ç”¨äºä¸å¹³è¡¡æ•°æ®ï¼Œâ˜…æ¨èâ˜…ï¼‰

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')

print(f"äº¤å‰éªŒè¯å¾—åˆ†: {scores.mean():.4f} Â± {scores.std():.4f}")

7.2 è¶…å‚æ•°ä¼˜åŒ–

ç½‘æ ¼æœç´¢ï¼ˆGridSearchCVï¼‰ï¼š
from sklearn.model_selection import GridSearchCV

# å®šä¹‰å‚æ•°ç½‘æ ¼

param_grid = {
'n_estimators': [50, 100, 200],
'max_depth': [3, 5, 7],
'learning_rate': [0.01, 0.1, 0.3]
}

# ç½‘æ ¼æœç´¢ï¼ˆç©·ä¸¾æ‰€æœ‰ç»„åˆï¼‰

grid_search = GridSearchCV(
estimator=GradientBoostingClassifier(),
param_grid=param_grid,
cv=5,
scoring='f1_macro',
n_jobs=-1,
verbose=1
)

grid_search.fit(X_train, y_train)

# æœ€ä½³å‚æ•°

print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹

best_model = grid_search.best_estimator_

éšæœºæœç´¢ï¼ˆRandomizedSearchCVï¼‰ï¼š
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# éšæœºæœç´¢ï¼ˆæ›´å¿«é€Ÿï¼Œé€‚åˆå¤§è§„æ¨¡å‚æ•°ç©ºé—´ï¼‰

param_dist = {
'n_estimators': randint(50, 200),
'max_depth': randint(3, 10),
'learning_rate': uniform(0.01, 0.3)
}

random_search = RandomizedSearchCV(
estimator=GradientBoostingClassifier(),
param_distributions=param_dist,
n_iter=30,  # éšæœºå°è¯•30æ¬¡
cv=5,
scoring='f1_macro',
n_jobs=-1,
random_state=42,
verbose=1
)

random_search.fit(X_train, y_train)

è´å¶æ–¯ä¼˜åŒ–ï¼ˆæ›´é«˜çº§ï¼‰ï¼š
from skopt import BayesSearchCV

# è´å¶æ–¯ä¼˜åŒ–ï¼ˆæ›´æ™ºèƒ½ï¼Œè‡ªåŠ¨å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼‰

search_space = {
'n_estimators': (50, 200),
'max_depth': (3, 10),
'learning_rate': (0.01, 0.3)
}

bayes_search = BayesSearchCV(
estimator=GradientBoostingClassifier(),
search_spaces=search_space,
n_iter=30,
cv=5,
scoring='f1_macro',
n_jobs=-1,
random_state=42
)

bayes_search.fit(X_train, y_train)

---

ğŸ”¬ é˜¶æ®µ8: è¯„ä¼°ä¸ä¼˜åŒ–

æ ¸å¿ƒä»»åŠ¡

- å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½
- æ£€æµ‹è¿‡æ‹Ÿåˆ
- æ¨¡å‹è§£é‡Šæ€§åˆ†æ
- æŒç»­ä¼˜åŒ–

æ¶‰åŠç®—æ³•/æŠ€æœ¯

8.1 è¯„ä¼°æŒ‡æ ‡è¯¦è§£

åˆ†ç±»é—®é¢˜ï¼š
from sklearn.metrics import (
accuracy_score, precision_score, recall_score, f1_score,
roc_auc_score, confusion_matrix, classification_report
)

# é¢„æµ‹

y_pred = model.predict(X_test)

# åŸºç¡€æŒ‡æ ‡

print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print(f"ç²¾ç¡®ç‡: {precision_score(y_test, y_pred, average='macro'):.4f}")
print(f"å¬å›ç‡: {recall_score(y_test, y_pred, average='macro'):.4f}")
print(f"F1åˆ†æ•°: {f1_score(y_test, y_pred, average='macro'):.4f}")

# è¯¦ç»†æŠ¥å‘Š

print(classification_report(y_test, y_pred))

# æ··æ·†çŸ©é˜µ

cm = confusion_matrix(y_test, y_pred)
print(cm)

# å¯è§†åŒ–æ··æ·†çŸ©é˜µ

import seaborn as sns
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('æ··æ·†çŸ©é˜µ')
plt.show()

å›å½’é—®é¢˜ï¼š
from sklearn.metrics import (
mean_squared_error, mean_absolute_error, r2_score
)

print(f"MSE: {mean_squared_error(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.4f}")
print(f"RÂ²: {r2_score(y_test, y_pred):.4f}")

8.2 è¿‡æ‹Ÿåˆæ£€æµ‹

æ–¹æ³•ä¸€ï¼šå­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
model, X_train, y_train,
cv=5, scoring='f1_macro',
train_sizes=np.linspace(0.1, 1.0, 10)
)

train_mean = np.mean(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)

plt.plot(train_sizes, train_mean, label='Training')
plt.plot(train_sizes, val_mean, label='Validation')
plt.legend()
plt.show()

# è§£è¯»ï¼š

# - ä¸¤æ¡çº¿æ¥è¿‘ â†’ æ— è¿‡æ‹Ÿåˆ

# - è®­ç»ƒçº¿è¿œé«˜äºéªŒè¯çº¿ â†’ è¿‡æ‹Ÿåˆ

# - ä¸¤æ¡çº¿éƒ½å¾ˆä½ â†’ æ¬ æ‹Ÿåˆ

æ–¹æ³•äºŒï¼šéªŒè¯æ›²çº¿
from sklearn.model_selection import validation_curve

param_range = np.arange(1, 20)
train_scores, val_scores = validation_curve(
model, X_train, y_train,
param_name='max_depth',
param_range=param_range,
cv=5, scoring='f1_macro'
)

train_mean = np.mean(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)

plt.plot(param_range, train_mean, label='Training')
plt.plot(param_range, val_mean, label='Validation')
plt.xlabel('max_depth')
plt.ylabel('Score')
plt.legend()
plt.show()

# è§£è¯»ï¼š

# - æ‰¾åˆ°éªŒè¯é›†æœ€é«˜ç‚¹ â†’ æœ€ä½³å¤æ‚åº¦

# - è¿‡è¯¥ç‚¹åéªŒè¯é›†ä¸‹é™ â†’ è¿‡æ‹Ÿåˆ

8.3 æ¨¡å‹è§£é‡Šï¼ˆå¯è§£é‡ŠAIï¼‰

SHAPå€¼ï¼ˆæœ€å¼ºå¤§ï¼‰ï¼š
import shap

# 1. è®¡ç®—SHAPå€¼

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# 2. æ€»ä½“ç‰¹å¾é‡è¦æ€§

shap.summary_plot(shap_values, X_test, feature_names=X.columns)

# 3. å•ä¸ªæ ·æœ¬è§£é‡Š

shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:])

# 4. ä¾èµ–å›¾ï¼ˆåˆ†æç‰¹å¾äº¤äº’ï¼‰

shap.dependence_plot('HGB', shap_values[0], X_test)

ç‰¹å¾é‡è¦æ€§ï¼ˆæ ‘æ¨¡å‹ï¼‰ï¼š
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title('Feature Importances')
plt.bar(range(len(importances)), importances[indices])
plt.xticks(range(len(importances)), X.columns[indices], rotation=90)
plt.show()

8.4 ROCæ›²çº¿ä¸PRæ›²çº¿

from sklearn.metrics import roc_curve, auc, precision_recall_curve

# ROCï¼ˆäºŒåˆ†ç±»ï¼‰

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# PRæ›²çº¿ï¼ˆä¸å¹³è¡¡æ•°æ®æ›´ä¼˜ï¼‰

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)

plt.plot(recall, precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

---

ğŸš€ é˜¶æ®µ9: éƒ¨ç½²ä¸ç›‘æ§

æ ¸å¿ƒä»»åŠ¡

- æ¨¡å‹åºåˆ—åŒ–ä¿å­˜
- æ„å»ºé¢„æµ‹API
- æŒç»­ç›‘æ§æ€§èƒ½
- æ¨¡å‹æ›´æ–°è¿­ä»£

æ¶‰åŠç®—æ³•/æŠ€æœ¯

9.1 æ¨¡å‹ä¿å­˜ä¸åŠ è½½

import pickle
import joblib

# æ–¹æ³•1ï¼špickleï¼ˆæ ‡å‡†ï¼Œä½†å¯èƒ½æœ‰å®‰å…¨é—®é¢˜ï¼‰

with open('model.pkl', 'wb') as f:
pickle.dump(model, f)

with open('model.pkl', 'rb') as f:
loaded_model = pickle.load(f)

# æ–¹æ³•2ï¼šjoblibï¼ˆæ¨èï¼Œå¯¹å¤§æ•°æ®æ›´é«˜æ•ˆï¼‰

joblib.dump(model, 'model.joblib')
loaded_model = joblib.load('model.joblib')

# æ–¹æ³•3ï¼šONNXï¼ˆè·¨å¹³å°ï¼Œç”Ÿäº§ç¯å¢ƒæ¨èï¼‰

# pip install onnx sklearn-onnx

from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]
onnx_model = convert_sklearn(model, initial_types=initial_type)

with open("model.onnx", "wb") as f:
f.write(onnx_model.SerializeToString())

9.2 æ„å»ºé¢„æµ‹APIï¼ˆFlaskç¤ºä¾‹ï¼‰

from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)
model = joblib.load('anemia_model.joblib')

@app.route('/predict', methods=['POST'])
def predict():
# è·å–JSONæ•°æ®
data = request.get_json()

  # è½¬æ¢ä¸ºDataFrame
  df = pd.DataFrame([data])

  # é¢„å¤„ç†ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ç›¸åŒï¼ï¼‰
  df = preprocess(df)  # éœ€è¦å®ç°ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é¢„å¤„ç†

  # é¢„æµ‹
  prediction = model.predict(df)

  # è¿”å›ç»“æœ
  return jsonify({
      'prediction': int(prediction[0]),
      'confidence': float(model.predict_proba(df).max())
  })
if __name__ == '__main__':
app.run(host='0.0.0.0', port=5000)

9.3 ç”Ÿäº§çº§éƒ¨ç½²ï¼ˆFastAPIæ›´ä½³ï¼‰

from fastapi import FastAPI
from pydantic import BaseModel
import numpy as np
import joblib

app = FastAPI(title="è´«è¡€è¯Šæ–­API")

# è¾“å…¥æ•°æ®æ¨¡å‹

class BloodTestData(BaseModel):
HGB: float
RBC: float
MCV: float
MCH: float
MCHC: float
RDW: float
# ... å…¶ä»–ç‰¹å¾

# åŠ è½½æ¨¡å‹

model = joblib.load('anemia_model.joblib')

@app.post("/predict")
async def predict(data: BloodTestData):
# è½¬æ¢ä¸ºæ•°ç»„
features = np.array([[
data.HGB, data.RBC, data.MCV, data.MCH,
data.MCHC, data.RDW,
# ... å…¶ä»–ç‰¹å¾
]])

  # é¢„æµ‹
  prediction = model.predict(features)
  probability = model.predict_proba(features)

  # ç±»åˆ«æ˜ å°„
  class_names = ['æ— è´«è¡€', 'HGBè´«è¡€', 'ç¼ºé“æ€§è´«è¡€',
                 'å¶é…¸ç¼ºä¹', 'B12ç¼ºä¹']

  return {
      "diagnosis": class_names[prediction[0]],
      "confidence": float(probability[0].max()),
      "probabilities": {
          class_names[i]: float(prob)
          for i, prob in enumerate(probability[0])
      }
  }
# è®¿é—®æ–‡æ¡£: http://localhost:8000/docs

9.4 æ¨¡å‹ç›‘æ§

# æ—¥å¿—è®°å½•é¢„æµ‹

import logging
from datetime import datetime

logging.basicConfig(filename='predictions.log', level=logging.INFO)

def log_prediction(input_data, prediction, confidence):
log_entry = {
'timestamp': datetime.now().isoformat(),
'input': input_data,
'prediction': prediction,
'confidence': confidence
}
logging.info(json.dumps(log_entry))

# A/Bæµ‹è¯•

def ab_test(model_a, model_b, X, ratio=0.5):
if np.random.random() < ratio:
return model_a.predict(X), 'A'
else:
return model_b.predict(X), 'B'

# æ€§èƒ½é€€åŒ–æ£€æµ‹

def detect_model_drift(recent_predictions, ground_truth, threshold=0.05):
recent_accuracy = accuracy_score(ground_truth, recent_predictions)
original_accuracy = 0.9997  # è®­ç»ƒæ—¶å‡†ç¡®ç‡

  if (original_accuracy - recent_accuracy) > threshold:
      print(f"è­¦å‘Š: æ¨¡å‹æ€§èƒ½ä¸‹é™ {original_accuracy - recent_accuracy:.2%}")
      return True
  return False
---

ğŸ“‹ å®Œæ•´æµç¨‹æ€»ç»“è¡¨


| é˜¶æ®µ        | æ ¸å¿ƒä»»åŠ¡ | å…³é”®ç®—æ³•/æŠ€æœ¯       | æ—¶é—´å æ¯” | å¸¸è§é™·é˜±     |
| ----------- | -------- | ------------------- | -------- | ------------ |
| 1. é—®é¢˜å®šä¹‰ | æ˜ç¡®ç›®æ ‡ | é—®é¢˜ç±»å‹è¯†åˆ«        | 5%       | ç›®æ ‡æ¨¡ç³Š     |
| 2. æ•°æ®æ”¶é›† | è·å–æ•°æ® | SQL/API/çˆ¬è™«        | 10%      | æ•°æ®æ³„éœ²     |
| 3. é¢„å¤„ç†   | æ¸…æ´—æ•°æ® | ç¼ºå¤±å€¼/å¼‚å¸¸å€¼å¤„ç†   | 25%      | ä¿¡æ¯ä¸¢å¤±     |
| 4. EDA      | æ¢ç´¢åˆ†æ | å¯è§†åŒ–/ç»Ÿè®¡         | 10%      | è¿‡åº¦è§£è¯»     |
| 5. ç‰¹å¾å·¥ç¨‹ | æ„é€ ç‰¹å¾ | ç¼–ç /é€‰æ‹©/è½¬æ¢      | 25%      | ç‰¹å¾æ³„éœ²     |
| 6. æ¨¡å‹é€‰æ‹© | é€‰åŸºçº¿   | å¤šç®—æ³•å¯¹æ¯”          | 5%       | å¤æ‚æ¨¡å‹åå¥½ |
| 7. è®­ç»ƒ     | è°ƒå‚     | äº¤å‰éªŒè¯/GridSearch | 10%      | è¿‡æ‹Ÿåˆ       |
| 8. è¯„ä¼°     | éªŒè¯     | ROC/PR/SHAP         | 5%       | å•ä¸€æŒ‡æ ‡     |
| 9. éƒ¨ç½²     | ä¸Šçº¿     | API/ç›‘æ§            | 5%       | ç¯å¢ƒå·®å¼‚     |

æ€»è€—æ—¶åˆ†é…ï¼š

- æ•°æ®ç›¸å…³ï¼ˆ2+3+4+5ï¼‰ï¼š70% â­
- å»ºæ¨¡ç›¸å…³ï¼ˆ6+7+8ï¼‰ï¼š20%
- éƒ¨ç½²ï¼ˆ9ï¼‰ï¼š10%

---

ğŸ’¡ å®æˆ˜å»ºè®®

1. è¿­ä»£æ€ç»´

ä¸è¦ï¼š1â†’2â†’3â†’...â†’9ï¼ˆä¸€æ¬¡æ€§å®Œæˆï¼‰

è¦ï¼š1â†’2â†’3â†’2â†’3â†’4â†’3â†’5â†’7â†’8â†’5â†’7â†’...ï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
â†‘           â†‘
å‘ç°é—®é¢˜     ä¼˜åŒ–ç‰¹å¾

2. å·¥å…·æ¨è


| å·¥å…·             | ç”¨é€”     | æ¨èç†ç”±           |
| ---------------- | -------- | ------------------ |
| JupyterLab       | å®éªŒå¼€å‘ | äº¤äº’å¼ã€æ”¯æŒå¯è§†åŒ– |
| PyCharm          | ç”Ÿäº§ä»£ç  | è°ƒè¯•å¼ºå¤§ã€ä»£ç è§„èŒƒ |
| Git              | ç‰ˆæœ¬æ§åˆ¶ | å¯è¿½æº¯ã€åä½œå¿…å¤‡   |
| MLflow           | å®éªŒç®¡ç† | è‡ªåŠ¨è®°å½•å‚æ•°å’Œç»“æœ |
| Weights & Biases | å®éªŒè·Ÿè¸ª | å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹     |

3. å¸¸è§é™·é˜±ä¸é¿å…æ–¹æ³•


| é™·é˜±         | è¡¨ç°                   | è§£å†³æ–¹æ¡ˆ                        |
| ------------ | ---------------------- | ------------------------------- |
| æ•°æ®æ³„éœ²     | æµ‹è¯•é›†å¾—åˆ†å¼‚å¸¸é«˜       | ä¸¥æ ¼åˆ†ç¦»è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®      |
| è¿‡æ‹Ÿåˆ       | è®­ç»ƒ>>æµ‹è¯•å¾—åˆ†         | å¢åŠ æ­£åˆ™åŒ–ã€ç®€åŒ–æ¨¡å‹ã€äº¤å‰éªŒè¯  |
| é€‰æ‹©åå·®     | åªåœ¨å¥½æ•°æ®ä¸Šæµ‹è¯•       | ç¡®ä¿éªŒè¯é›†ä»£è¡¨çœŸå®åˆ†å¸ƒ          |
| æŒ‡æ ‡è¯¯å¯¼     | å‡†ç¡®ç‡å¾ˆé«˜ä½†ä¸šåŠ¡æŒ‡æ ‡å·® | ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ï¼ˆF1ã€AUCã€Recallï¼‰ |
| ç‰¹å¾å·¥ç¨‹è¿‡åº¦ | æ•°ç™¾ä¸ªç‰¹å¾ä½†æ€§èƒ½ä¸æå‡ | å…³æ³¨é¢†åŸŸçŸ¥è¯†ï¼Œé¿å…æš´åŠ›ç»„åˆ      |

---

ğŸ¯ è´«è¡€é¡¹ç›®å…¨æµç¨‹å›é¡¾

ç»“åˆæ‚¨çš„å®é™…é¡¹ç›®ï¼Œå›é¡¾ä¸€ä¸‹å®Œæ•´æµç¨‹ï¼š

1. é—®é¢˜å®šä¹‰ âœ…
   "æ„å»ºå¤šåˆ†ç±»æ¨¡å‹ï¼Œæ ¹æ®24é¡¹è¡€æ¶²æŒ‡æ ‡é¢„æµ‹5ç§è´«è¡€ç±»å‹"
2. æ•°æ®æ”¶é›† âœ…
   Kaggleä¸‹è½½ï¼Œ15,300æ¡è®°å½•
3. é¢„å¤„ç† âœ…

   - åˆ é™¤é‡å¤å€¼ï¼ˆ88æ¡ï¼‰
   - å¤„ç†ç¼ºå¤±å€¼ï¼ˆä¸­ä½æ•°å¡«å……ï¼‰
   - ä¿ç•™å¼‚å¸¸å€¼ï¼ˆåŒ»å­¦ç‰¹å¾ï¼‰
4. EDA âœ…

   - ç»Ÿè®¡åˆ†æ
   - å¯è§†åŒ–
   - ç›¸å…³æ€§åˆ†æ
5. ç‰¹å¾å·¥ç¨‹ âœ…

   - 24ä¸ªåŸå§‹ç‰¹å¾
   - Min-Maxå½’ä¸€åŒ–
   - æ— æ–°ç‰¹å¾åˆ›é€ ï¼ˆåŒ»å­¦æŒ‡æ ‡å·²å¾ˆä¸“ä¸šï¼‰
6. æ¨¡å‹é€‰æ‹© âœ…

   - å¯¹æ¯”6ç§ç®—æ³•
   - é€‰æ‹©Gradient Boosting
7. è®­ç»ƒ âœ…

   - 5æŠ˜äº¤å‰éªŒè¯
   - hyperparameter tuningï¼ˆå¦‚éœ€è¦ï¼‰
8. è¯„ä¼° âœ…

   - å‡†ç¡®ç‡99.97%
   - Macro F1: 99.94%
   - æ— è¿‡æ‹Ÿåˆ
   - ç‰¹å¾é‡è¦æ€§åˆ†æ
9. éƒ¨ç½² â¸ï¸ï¼ˆå¯é€‰ï¼‰

   - ä¿å­˜æ¨¡å‹
   - APIå°è£…
   - ç›‘æ§ç³»ç»Ÿ

æ‚¨çš„é¡¹ç›®å·²ç»å®Œæˆäº†å‰8ä¸ªé˜¶æ®µï¼Œå¹¶ä¸”åšå¾—éå¸¸ä¼˜ç§€ï¼å¦‚æœ‰éœ€è¦ï¼Œå¯éšæ—¶è¿›å…¥ç¬¬9é˜¶æ®µéƒ¨ç½²ä¸Šçº¿ã€‚

---

ğŸ“š å­¦ä¹ èµ„æº

- ä¹¦ç±: ã€ŠHands-On Machine Learning with Scikit-Learn, Keras & TensorFlowã€‹
- åœ¨çº¿è¯¾ç¨‹: Courseraã€ŠMachine Learningã€‹ï¼ˆå´æ©è¾¾ï¼‰
- å®˜æ–¹æ–‡æ¡£: scikit-learnå®˜æ–¹æ–‡æ¡£ï¼ˆæœ‰ä¸­æ–‡ç¿»è¯‘ï¼‰
- å®æˆ˜å¹³å°: Kaggleç«èµ›ã€é˜¿é‡Œå¤©æ± 

---
